---
layout: post
title:  "The Road to a Prototype. 8. Camera."
date:   2017-05-26
tags: ['article', 'prototyping', 'roguelike', 'sdl', 'camera', 'system', 'rendering']
author: "Hugo Viala"
---

Welcome to *The Road to a Protoype*, a blog series in which I weekly describe my attempt at prototyping a small game all by myself.

# Previously in...

Last week I decided to change the way the map worked and went for a thing I really liked which was using hexagonal tile maps. Now I have a setup I can rely on and expand upon. But now, before actually putting events and resources on each individual tiles, there was one last thing that I wanted to add to the engine to navigate more easily in the map : a camera system.

# Requirements

I like to have precise requirements when I implement something, even though I do not always explicitly write them down. In this case, the requirements were :

1. Being able to zoom in and out of the map
2. Scroll and drag/drop the map
3. The camera only looks directly at the map (no angle between camera and map plane)

The second point was largely implemented previously in a small system that I wrote to record mouse drag/drop, the third point basically meant a simplification for all computations (a simplication that I could rely upon, but was not forced to). And with that, I was equipped toward getting all the stuff done.

Just a quick note, I assume here that you somewhat know already a little bit about camera systems, how they work and how their coordinate system is setup since I cannot explain it all in one blog post.

# Systems are already there

First, what kind of data do I need. I had already implemented a camera system previously on a 3d renderer I am working on called [gliewer](https://github.com/rivten/gliewer). Because of that, I thought it would be a very easy task, but I quickly noticed that I could not fully reuse the camera system implemented in the 3d renderer mostly because the basic assumptions differs between the two projects. Indeed one is fundamentally 2d when the other is fundamentally 3d. It is therefore no surprise that camera systems differ.

So here is how the camera system in this project is setup.

First, the camera structure.

{% highlight c++ %}
struct camera
{
	v3 P;
	v3 XAxis;
	v3 ZAxis;
	float FocalLength;
};
{% endhighlight %}

Very simple, once again. We have position, XAxis and ZAxis are for orientation (with my third requirement I could get rid of them since they would be aligned with world coordinates) and the focal length to know the plane equation to render to.

Obviously, this comes with a function to create a camera.

# Renderer insertion

Now, the camera data should be fed to the renderer so that the latter knows how to modify coordinates before actually rendering them.

In case you do not remember, here is how the rendering pipeline basically worked up to now :

1. The user, in gameplay code, pushes *rendering commands* onto the renderer's buffer
2. At the end of the frame, the renderer goes through each command in the buffer.
3. For each rendering command, the renderer call the appropriate SDL function to render it on screen.

Here is how this pipeline became with the camera system introduction

1. The user, in gameplay code, pushes *rendering commands* onto the renderer's buffer
2. At the end of the frame, the renderer goes through each command in the buffer.
3. For each rendering command,
	1. Get the data from the command
	2. Project all the coordinates from world coordinates to window coordinates thanks to camera data
	3. Call the appropriate SDL function to render on screen.

For example, here is one part of the render function right now :

{% highlight c++ %}
void Render(renderer* Renderer)
{
	for(u32 CommandIndex = 0; CommandIndex < Renderer->CommandCount; ++CommandIndex)
	{
		render_command* Command = (render_command *)Renderer->Arena->Base + CommandIndex;
		switch(Command->Type)
		{
			case RenderCommand_Line:
				{
					v2 Begin = Command->Begin;
					v2 End = Command->End;

					if(!FlagSet(Command->Flags, RenderCommandFlag_UI))
					{
						Begin = Projection(Renderer, Begin);
						End = Projection(Renderer, End);
					}
					SDLSetRenderColor(Renderer->SDLContext, 
							Command->LineColor);
					SDL_RenderDrawLine(Renderer->SDLContext, 
							Begin.x, 
							Renderer->WindowSizeInPixels.y - Begin.y,
							End.x, 
							Renderer->WindowSizeInPixels.y - End.y);
				} break;
			case RenderCommand_Hex:
				{
					if(!FlagSet(Command->Flags, RenderCommandFlag_UI))
					{
						Command->Radius = 
							(Renderer->Camera->FocalLength * 
								Command->Radius) / 
									Renderer->Camera->P.z;
						Command->Center = Projection(Renderer, 
							Command->Center);
					}
					SDLDrawHexagon(Renderer->SDLContext, 
							SwitchBetweenWindowRenderCoords(Renderer, 
										Command->Center), 
							Command->Radius, 
							Command->HexColor, 
							Command->InitialAngle);
				} break;
			// NOTE(hugo) : Other command types
		}
	}

	SDL_RenderPresent(Renderer->SDLContext);
}
{% endhighlight %}

As you can see, now a rendering command have several flags. One possible flag is stating whether this command was meant to display UI or not. This is important because UI command do not need to be projected on the screen.
The projection function is pretty simple : it relies upon building the LookAt matrix from the camera and then uses basic Thales theorem to compute the final position on the rendering plane (given by the focal length).

{% highlight c++ %}
v2 Projection(renderer* Renderer, v2 P)
{
	camera* Camera = Renderer->Camera;

	mat4 View = LookAt(Camera->P, Camera->XAxis, Camera->ZAxis);
	v4 PosCamSpace = View * V4(P.x, P.y, 0.0f, 1.0f);
	Assert(PosCamSpace.w != 0.0f);
	PosCamSpace.xy = PosCamSpace.xy / PosCamSpace.w;
	v2 Result = (Camera->FocalLength / Camera->P.z) * PosCamSpace.xy;

	Result += 0.5f * V2(GlobalWindowWidth, GlobalWindowHeight);

	return(Result);
}
{% endhighlight %}

As you may have noticed, the LookAt matrix is indeed recomputed each time a command is processed, so there is a loooot of wasted computation time right here, this is noted but, as the command count is not very high at the moment, I do not plan on doing anything here until I reach a perf issue, then I will have all the information in hand to take the right decision. Classy.

That is basically it for inserting the camera system into the rendering one. Then, it is just a matter of responding to user input to change the camera height to take a higher or a closer view to the map.

{% highlight c++ %}
if(Pressed(Input->Keyboard.Buttons[SCANCODE_KP_PLUS]))
{
	GameState->Camera.P.z += 0.2f;
}
if(Pressed(Input->Keyboard.Buttons[SCANCODE_KP_MINUS]))
{
	GameState->Camera.P.z -= 0.2f;
}
{% endhighlight %}

![road_sc_801](/images/proto_road/road_sc_801.png)
<center>Tile map seen from a higher point of view</center>

# A little bonus

That is basically it. Pretty simple, eh ?

And that system is also pretty flexible too. Imagine I want the camera to constantly follow the player ? Well, nothing is simper : just add a small piece of code whenever the player make a move and you are good to go.

{% highlight c++ %}
bool Move(player* Player, hex_tile* DestTile, game_state* GameState)
{
	bool Result = false;
	if(AreHexTileNeighbors(DestTile, GameState->Player.Tile))
	{
		Result = true;
		GameState->Player.Tile = DestTile;

		// NOTE(hugo): This is just an example of camera tracking
		GameState->Camera.P.xy = GetPlayerPosition(GameState->Player, 
				GameState->TileMapData);

		// NOTE(hugo) : Then do movement stuff
		// [...]
	}

	return(Result);
}
{% endhighlight %}

# Conclusion

So that was pretty simple to get a quite flexible system.
Something that ended up not very great in the final implementation was clicking on stuff. Indeed, when you click somewhere in the screen, the mouse coordinate is in window space. So a comparison have to be made between the mouse's coordinate system and the tiles (for example) coordinate system (which is world space). I decided to unproject the mouse coordinates and put it in world space each time a click is made. It works for now but I am unsure about how good it fits to the whole codebase. We will see when we will be able to click more extensively on objects.

But now, I have to focus on what is next. World procedural generation is now at hand and seems one of the most logical feature to do next. Basic gameplay and resource management for the player could also be something to take into account. I will think about this and decide which way I look forward to the most.

But let's get to programming !
